{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extraction\n",
    "import general\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "GNM_PATH = os.path.dirname(os.getcwd()) + '\\\\data\\\\database_dump_drugs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n = 50  # number of articles\n",
    "train_new = True  # create new (True) or use existing (False) pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 articles.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "input should be either str, list or Document",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-93296e50b669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneral\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_gnm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGNM_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'article_contentRaw'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparsed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-93296e50b669>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneral\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_gnm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGNM_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'article_contentRaw'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparsed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         assert any([isinstance(doc, str), isinstance(doc, list),\n\u001b[1;32m--> 252\u001b[1;33m                     isinstance(doc, Document)]), 'input should be either str, list or Document'\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: input should be either str, list or Document"
     ]
    }
   ],
   "source": [
    "raw_data = general.load_gnm(data_path = GNM_PATH, n = n, cols = ['article_contentRaw'])\n",
    "pipeline = extraction.create_pipeline()\n",
    "parsed_data = [pipeline(text) for doc in raw_data for text in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 10654 eventive nouns.\n"
     ]
    }
   ],
   "source": [
    "events = extraction.event_extraction(docs = parsed_data, type = \"Chambers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new:\n",
    "    df = general.load_data(n)\n",
    "\n",
    "    pipeline = extraction.create_pipeline()\n",
    "    raw_data = [pipeline(doc) for doc in df]\n",
    "    events = extraction.event_extraction(docs = raw_data, type = \"Chambers\")[1]\n",
    "\n",
    "    pmi_matrix, flat_events, sorted_set = extraction.get_pmi_matrix(events)  # flat_events and sorted_set serve as the keys to the pmi matrix rows and columns\n",
    "\n",
    "    # intermediate save \n",
    "    with open(\"flat_events_3.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(flat_events, fp)\n",
    "\n",
    "    with open(\"sorted_set_3.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(sorted_set, fp)\n",
    "\n",
    "    with open(\"pmi_matrix_3.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(pmi_matrix, fp)\n",
    "    \n",
    "else:\n",
    "    with open(\"flat_events_3.txt\", \"rb\") as fp:   # Unpickling\n",
    "        flat_events = pickle.load(fp)\n",
    "\n",
    "    with open(\"sorted_set_3.txt\", \"rb\") as fp:   # Unpickling\n",
    "        sorted_set = pickle.load(fp)\n",
    "\n",
    "    with open(\"pmi_matrix_3.txt\", \"rb\") as fp:   # Unpickling\n",
    "        pmi_matrix = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER 0\n",
      "challenge\n",
      "coincidence\n",
      "conclude\n",
      "crossing\n",
      "developing\n",
      "escalation\n",
      "estimated\n",
      "find\n",
      "gun\n",
      "ignoring\n",
      "left\n",
      "maintenance\n",
      "pick\n",
      "pressure\n",
      "revealed\n",
      "said\n",
      "selling\n",
      "shaped\n",
      "spend\n",
      "taking\n",
      "thanks\n",
      "turns\n",
      "tweak\n",
      "used\n",
      "visit\n",
      "warned\n",
      "CLUSTER 1\n",
      "bill\n",
      "business\n",
      "claiming\n",
      "collaborates\n",
      "conclusion\n",
      "cut\n",
      "direction\n",
      "discusses\n",
      "dubbed\n",
      "ensuring\n",
      "exchange\n",
      "flows\n",
      "goes\n",
      "grow\n",
      "indicate\n",
      "issued\n",
      "learned\n",
      "miss\n",
      "numbers\n",
      "pelting\n",
      "punishment\n",
      "receiving\n",
      "responsibility\n",
      "spends\n",
      "stalled\n",
      "suggested\n",
      "triggered\n",
      "writing\n",
      "CLUSTER 2\n",
      "brought\n",
      "checked\n",
      "comparison\n",
      "course\n",
      "detected\n",
      "escaped\n",
      "giving\n",
      "groaned\n",
      "growth\n",
      "knew\n",
      "lacking\n",
      "participated\n",
      "paying\n",
      "rescue\n",
      "resolution\n",
      "responded\n",
      "reverted\n",
      "say\n",
      "scribbled\n",
      "stop\n",
      "time\n",
      "trade\n",
      "trafficking\n",
      "treatment\n",
      "tripled\n",
      "wasting\n",
      "witnessed\n",
      "CLUSTER 3\n",
      "continue\n",
      "created\n",
      "decriminalization\n",
      "depends\n",
      "determined\n",
      "directed\n",
      "dry\n",
      "escorting\n",
      "field\n",
      "gave\n",
      "impose\n",
      "jerking\n",
      "keeping\n",
      "leaving\n",
      "medicine\n",
      "operations\n",
      "parked\n",
      "protesting\n",
      "received\n",
      "regulate\n",
      "reinforcing\n",
      "residence\n",
      "set\n",
      "snuff\n",
      "spits\n",
      "surge\n",
      "taught\n",
      "transporting\n",
      "wean\n",
      "CLUSTER 4\n",
      "brings\n",
      "cause\n",
      "collection\n",
      "contributing\n",
      "demonstrate\n",
      "drove\n",
      "ending\n",
      "entered\n",
      "fair\n",
      "fighting\n",
      "helping\n",
      "imprisoned\n",
      "littering\n",
      "mention\n",
      "predicated\n",
      "proved\n",
      "reference\n",
      "rejected\n",
      "saving\n",
      "snuffed\n",
      "suspended\n",
      "vowed\n",
      "CLUSTER 5\n",
      "building\n",
      "choose\n",
      "compiled\n",
      "considered\n",
      "controlled\n",
      "culture\n",
      "demand\n",
      "derive\n",
      "dropping\n",
      "employed\n",
      "felt\n",
      "followed\n",
      "importation\n",
      "infection\n",
      "involving\n",
      "joined\n",
      "masterminded\n",
      "modified\n",
      "observed\n",
      "proposal\n",
      "raises\n",
      "referred\n",
      "signal\n",
      "stealing\n",
      "survival\n"
     ]
    }
   ],
   "source": [
    "# clustering\n",
    "clustering = AgglomerativeClustering(n_clusters = None, affinity='precomputed', linkage='complete', distance_threshold = 15).fit(pmi_matrix)\n",
    "# add cluster labels to the sorted set\n",
    "clustered_set = list(zip(sorted_set, clustering.labels_))\n",
    "\n",
    "for x in range(6):\n",
    "    print(f'CLUSTER {x}')\n",
    "    for ev in clustered_set:\n",
    "        if ev[1] == x:\n",
    "            print(ev[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8974fa486e25e371e273e38d1ab5cf156e050a365dc77f37adcc2ebc054f7da"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
