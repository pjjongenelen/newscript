{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import stanza\n",
    "import warnings\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Head word extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_words(text, cl):\n",
    "    \"\"\"\n",
    "    Extracts single-noun head words from an article.\n",
    "\n",
    "    1) Extracts all noun phrases, tagged with 'NP' by the CoreNLPClient,\n",
    "    2) Removes multiples (NPs that contain other NPS),\n",
    "    3) Transforms multi-word NPs into single-word head nouns based on a simple Stanza Pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    text (list): Sentence-tokenized article\n",
    "    cl (CoreNLPClient)\n",
    "\n",
    "    Returns:\n",
    "    list: Head word sentence IDs & word IDs\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the noun phrases (tregex) and their indices    \n",
    "    matches = cl.tregex(text=text, pattern='NP')\n",
    "\n",
    "    noun_phrases = [[text, begin, end] for text, begin, end in\n",
    "                zip([sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence],\n",
    "                    [sentence[match_id]['characterOffsetBegin'] for sentence in matches['sentences'] for match_id in sentence],\n",
    "                    [sentence[match_id]['characterOffsetEnd'] for sentence in matches['sentences'] for match_id in sentence])]\n",
    "\n",
    "    # remove multiples\n",
    "    multiples = []\n",
    "    for text1, i1, i2 in noun_phrases:\n",
    "        for text2, j1, j2 in noun_phrases:\n",
    "            if (text1, i1, i2 != text2, j1, j2) and ((i1 >= j1) and (i2 <= j2)):\n",
    "                # if the text is not the same, and text1 is encapsulated in text2\n",
    "                multiples.append([text2, j1, j2])\n",
    "    noun_phrases = [[text, i1, i2] for [text, i1, i2] in noun_phrases if [text, i1, i2] not in multiples]\n",
    "\n",
    "    # turn multi-word noun phrases into single head words\n",
    "    head_words = []\n",
    "\n",
    "    for [t, i1, i2] in noun_phrases:\n",
    "        for sent in nlp(t).sentences:\n",
    "            for word in sent.words:\n",
    "                if word.deprel == \"root\":\n",
    "                    _ = i1 + t.find(word.text)  # start index of the root word in the original sentence\n",
    "                    head_words.append([_, _ + len(word.text)])\n",
    "\n",
    "    doc = nlp(text)\n",
    "    head_words = [[sent.id, word.id] for sent in doc.sentences for word in sent.words for [i1, i2] in head_words if word.start_char == i1 and word.end_char == i2]\n",
    "    \n",
    "    return head_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Trigger extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triggers(sents, hw):\n",
    "    \"\"\"\n",
    "    Extracts head words that have either a verb or an eventive noun as its subject/object/preposition.\n",
    "\n",
    "    1) Extract all verbs\n",
    "    2) Get eventive nouns based on the Wordnet Synsets indicated by the authors\n",
    "    3) Combine 1 and 2 into a list of trigger candidates\n",
    "    4) Finds all head words that have a trigger as its subject/object/preposition.\n",
    "    5) Looks for transitive triggers, and extracts the correct subject from the root verb.\n",
    "\n",
    "    Parameters:\n",
    "    sents (list): sentence-tokenized article\n",
    "    head_words (list): list of head word IDs\n",
    "\n",
    "    Returns:\n",
    "    list: Triplet lists of head word, trigger, and relation - e.g., [[4, 6, 'nsubj']]\n",
    "    \"\"\"\n",
    "\n",
    "    # parse the text and extract all verbs and lemmas\n",
    "    doc = nlp(text)\n",
    "    verbs = [[sent.id, word.id] for sent in doc.sentences for word in sent.words if word.upos == 'VERB']\n",
    "    lemmas = [[word.lemma, sent.id, word.id] for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "    # generate a list of eventive nouns\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        wn_evnouns = list(set([w for s in wn.synset('event.n.01').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "        wn_evnouns += list(set([w for s in wn.synset('act.n.02').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "\n",
    "    # generates a list of trigger candidates based on the verbs and eventive nouns in the text\n",
    "    candidates = verbs + [[s,w] for [lemma, s, w] in lemmas if lemma in wn_evnouns and [s, w] not in verbs]\n",
    "\n",
    "    # finds all head word - trigger dyads and their syntactic relation\n",
    "    triggers = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if ([sent.id, word.head] in candidates) and ([sent.id, word.id] in hw):\n",
    "                if (\"IN\" in word.xpos):\n",
    "                    triggers.append([sent.id, word.id, word.head, word.xpos])\n",
    "                elif any(_ in word.deprel for _ in [\"subj\", \"obj\"]):\n",
    "                    triggers.append([sent.id, word.id, word.head, word.deprel])\n",
    "\n",
    "    for [sent, word] in verbs:\n",
    "        head = doc.sentences[sent].words[word-1].head\n",
    "        if head != 0:\n",
    "            for [_, noun, verb, rel] in triggers:\n",
    "                if verb == head and ('subj' in rel):\n",
    "                    triggers.append([sent, noun, word, rel])\n",
    "\n",
    "\n",
    "    return triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Attribute extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Add coreference information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load articles\n",
    "with open('database_dump_drugs/0.json') as file:\n",
    "    articles = json.load(file)\n",
    "    \n",
    "articles = list(set([a['article_content'] for a in articles]))\n",
    "\n",
    "\n",
    "# global variables\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing 1.1\n",
    "with CoreNLPClient(properties='corenlp_server-2e15724b8064491b.props', endpoint='http://localhost:8000', memory='8G', be_quiet=True) as client:\n",
    "    head_words = get_head_words(text='Chris Manning is a nice person. He also gives oranges to people. Chris is a good fellow.', cl=client)\n",
    "head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10192/831699437.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# testing 1.2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_triggers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msents\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhw\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhead_words\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# testing 1.2\n",
    "tr = get_triggers(sents=text, hw=head_words)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demonstration purposes:\n",
    "doc = nlp(text)\n",
    "print(f'Input text: {text}')\n",
    "for x in range(len(tr)):\n",
    "    print(f'Head word: {doc.sentences[tr[x][0]].words[tr[x][1]-1].text}\\t Trigger: {doc.sentences[tr[x][0]].words[tr[x][2]-1].text}\\t Relation: {tr[x][3]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "529c65a099ec80885ec2eff43b42ddd85d3b5611c57bcaf738fb7ba7dbdb85b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}