{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# imports\r\n",
    "import json\r\n",
    "import stanza\r\n",
    "import warnings\r\n",
    "\r\n",
    "from nltk.corpus import wordnet as wn\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "from stanza.server import CoreNLPClient"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Head word extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def get_head_words(sents, endpoint='8000'):\r\n",
    "    \"\"\"\r\n",
    "    Extracts single-noun head words from an article.\r\n",
    "\r\n",
    "    First extracts all noun phrases, tagged with 'NP' by the CoreNLPClient, then removes multiples (NPs that contain other NPS).\r\n",
    "    Finally transforms multi-word NPs into single-word head nouns based on a simple Stanza Pipeline, and looks up the respective word IDs.\r\n",
    "\r\n",
    "    Parameters:\r\n",
    "    sents (list): Sentence-tokenized article\r\n",
    "    endpoint (string): Port to use for the CoreNLPClient\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    list: Head word ids\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # extract the noun phrases (tregex) and their indices\r\n",
    "    with CoreNLPClient(properties='corenlp_server-2e15724b8064491b.props', endpoint=f'http://localhost:{endpoint}', memory='8G', be_quiet=True) as client:\r\n",
    "        matches = client.tregex(text=sents[0], pattern='NP')\r\n",
    "\r\n",
    "    noun_phrases = [[text, begin, end] for text, begin, end in\r\n",
    "                zip([sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence],\r\n",
    "                    [sentence[match_id]['characterOffsetBegin'] for sentence in matches['sentences'] for match_id in sentence],\r\n",
    "                    [sentence[match_id]['characterOffsetEnd'] for sentence in matches['sentences'] for match_id in sentence])]\r\n",
    "\r\n",
    "    # remove multiples\r\n",
    "    multiples = []\r\n",
    "    for text1, i1, i2 in noun_phrases:\r\n",
    "        for text2, j1, j2 in noun_phrases:\r\n",
    "            if (text1 != text2) and (i1, i2 != j1, j2):\r\n",
    "                if (i1 >= j1) & (i2 <= j2):\r\n",
    "                    multiples.append([text2, j1, j2])\r\n",
    "    noun_phrases = [[text, i1, i2] for [text, i1, i2] in noun_phrases if [text, i1, i2] not in multiples]\r\n",
    "    \r\n",
    "    # turn multi-word noun phrases into single head words\r\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse', verbose=False)\r\n",
    "    head_words = []\r\n",
    "    for [text, i1, i2] in noun_phrases:\r\n",
    "        for word in nlp(text.lower()).sentences[0].words:\r\n",
    "            if word.deprel == \"root\":\r\n",
    "                _ = i1 + text.lower().find(word.text)  # start index of the root word in the original sentence\r\n",
    "                head_words.append([_, _ + len(word.text)])\r\n",
    "\r\n",
    "    # swap the word indices for their respective ids\r\n",
    "    doc = nlp(sents[0])\r\n",
    "    head_words = [word.id for [i1, i2] in head_words for sent in doc.sentences for word in sent.words if word.start_char == i1 and word.end_char == i2]  \r\n",
    "\r\n",
    "    return head_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Trigger extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def get_triggers(sents, hw):\r\n",
    "    \"\"\"\r\n",
    "    Extracts head words that have either a verb or an eventive noun as its subject/object/preposition.\r\n",
    "\r\n",
    "    First extracts all triggers: verbs or eventive nouns based on the Wordnet Synsets indicated by the authors.\r\n",
    "    Then goes through all head words in hw, and finds those that have a trigger as its subject/object/preposition\r\n",
    "\r\n",
    "    Parameters:\r\n",
    "    sents (list): sentence-tokenized article\r\n",
    "    head_words (list): list of head word IDs\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    list: Triplet lists of head word, trigger, and relation - e.g., [[4, 6, 'nsubj:pass']]\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    triggers = []\r\n",
    "    # get a list of all verbs in the article\r\n",
    "    doc = nlp(sents[0])\r\n",
    "    evnouns = []\r\n",
    "    for sent in doc.sentences:\r\n",
    "        for word in sent.words:\r\n",
    "            evnouns.append([word.lemma, word.id])\r\n",
    "            if word.upos == 'VERB':\r\n",
    "                triggers.append(word.id)\r\n",
    "\r\n",
    "    # get 'eventive' nouns from the two wordnet synsets mentioned in the article and add these to the list of verbs, preventing duplicates\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter(\"ignore\")\r\n",
    "        wn_evnouns = list(set([w for s in wn.synset('event.n.01').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\r\n",
    "        wn_evnouns += list(set([w for s in wn.synset('act.n.02').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\r\n",
    "    triggers = sorted(triggers + [i for [lemma, i] in evnouns if lemma in wn_evnouns and i not in triggers])\r\n",
    "    \r\n",
    "    # find all head words that are related to a trigger\r\n",
    "    # and clean up the list, keeping either the deprel or xpos, depening on the type of relation between head word and trigger\r\n",
    "    triggers = [[word.id, word.head, word.deprel, word.xpos] for sent in doc.sentences for word in sent.words if ((\"subj\" in word.deprel) or (\"obj\" in word.deprel) or (\"IN\" in word.xpos)) and (word.head in triggers) and (word.id in hw)]\r\n",
    "    triggers = [[id1, head, rel_dep] if ((\"subj\" in rel_dep) or (\"obj\" in rel_dep)) else [id1, head, rel_xpos] for [id1, head, rel_dep, rel_xpos] in triggers]\r\n",
    "\r\n",
    "    return triggers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Extract attributes of the remaining head words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing section 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# load sample article\r\n",
    "with open('database_dump_drugs/0.json') as file:\r\n",
    "    article = json.load(file)[0]['article_content'][:423]\r\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse', verbose=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# testing 1.1\r\n",
    "sentences = sent_tokenize(article)\r\n",
    "head_words = get_head_words(sents=sentences)\r\n",
    "print(head_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4, 8, 12, 16, 18]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# testing 1.2\r\n",
    "tr = get_triggers(sents=sentences, hw=head_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# for demonstration purposes:\r\n",
    "doc = nlp(sentences[0])\r\n",
    "print(f'Input sentence: {sentences[0]}')\r\n",
    "print(f'Head word: {doc.sentences[0].words[tr[0][0]-1].text}\\t Trigger: {doc.sentences[0].words[tr[0][1]-1].text}\\t Relation: {tr[0][2]}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input sentence: An Ethiopian foreign national was arrested in eMzinoni for dealing in drugs at his tuck shop on Friday, 6 October.\n",
      "Head word: national\t Trigger: arrested\t Relation: nsubj:pass\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('thesis': conda)"
  },
  "interpreter": {
   "hash": "529c65a099ec80885ec2eff43b42ddd85d3b5611c57bcaf738fb7ba7dbdb85b1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}