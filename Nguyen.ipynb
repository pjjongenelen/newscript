{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# imports\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import signal\r\n",
    "import stanza\r\n",
    "import subprocess\r\n",
    "import time\r\n",
    "\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "from stanza.server import CoreNLPClient"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def clear_port_9000():\r\n",
    "    # clear the 9000 port to be used by the CoreNLPClient\r\n",
    "    command = \"netstat -ano | findstr 9000\"\r\n",
    "    c = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr = subprocess.PIPE)\r\n",
    "    stdout, stderr = c.communicate()\r\n",
    "    if len(stdout) > 0:    \r\n",
    "        # process is running on port 9000, terminate it\r\n",
    "        print(\"Netstat response:\")\r\n",
    "        print(stdout)\r\n",
    "        print(\"-----------------------------------------------------\")\r\n",
    "        pid = input(\"Enter the process ID of the process that should be terminated, or an alphanumeric character in case it is unnecessary.\")\r\n",
    "        if not pid.isalpha():\r\n",
    "            os.kill(int(pid), signal.SIGTERM)\r\n",
    "            time.sleep(5)  # wait for the process to close"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Head word extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "def get_head_words(sentences):\r\n",
    "    # extracts single-noun head words from a list of sentences\r\n",
    "\r\n",
    "    clear_port_9000()\r\n",
    "    with CoreNLPClient(properties='corenlp_server-2e15724b8064491b.props', memory='8G', be_quiet=True) as client:\r\n",
    "        matches = client.tregex(text=sentences[0], pattern='NP')  # finds the noun phrases in the given text\r\n",
    "\r\n",
    "    # extract the noun phrases and their indices\r\n",
    "    noun_phrases = [[text, begin, end] for text, begin, end in\r\n",
    "                zip([sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence],\r\n",
    "                    [sentence[match_id]['characterOffsetBegin'] for sentence in matches['sentences'] for match_id in sentence],\r\n",
    "                    [sentence[match_id]['characterOffsetEnd'] for sentence in matches['sentences'] for match_id in sentence])]\r\n",
    "\r\n",
    "    # find head words that contain other head words (let's call them multiples)\r\n",
    "    multiples = []\r\n",
    "    for text1, i1, i2 in noun_phrases:\r\n",
    "        for text2, j1, j2 in noun_phrases:\r\n",
    "            if (text1 != text2) and (i1, i2 != j1, j2):\r\n",
    "                if (i1 >= j1) & (i2 <= j2):\r\n",
    "                    multiples.append([text2, j1, j2])\r\n",
    "    noun_phrases = [[text, i1, i2] for [text, i1, i2] in noun_phrases if [text, i1, i2] not in multiples]\r\n",
    "\r\n",
    "    # turn multi-word noun phrases into single head words\r\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse', verbose=False)\r\n",
    "    [[text, i1, i2] for [text, i1, i2] in noun_phrases for sent in nlp(text.lower()).sentences for word in sent.words if word.deprel == \"root\"]\r\n",
    "\r\n",
    "    return noun_phrases"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Trigger extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_triggers():\r\n",
    "    triggers = []\r\n",
    "    # 1) get verbs\r\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos', verbose=False)\r\n",
    "    doc = nlp('Barack Obama was born in Hawaii. Tim Jongenelen was born in Zegge, but currently lives in Eindhoven after having moved from Roermond.')\r\n",
    "    for sent in doc.sentences:\r\n",
    "        for word in sent.words:\r\n",
    "            if word.upos == 'VERB':\r\n",
    "                triggers.append(word.text)\r\n",
    "\r\n",
    "    # 2) get 'eventive' nouns from the two wordnet synsets\r\n",
    "    # from the definitions and examples we can clearly see that we need the synsets 'event.n.01' and 'act.n.02'\r\n",
    "    events = list(set([w for s in wn.synset('event.n.01').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\r\n",
    "    acts = list(set([w for s in wn.synset('act.n.02').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\r\n",
    "\r\n",
    "    # 3) maintain only triggers where a head word serves as its subject, object or preposition\r\n",
    "\r\n",
    "    return triggers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Drop head words that are not related to at least one trigger"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Extract attributes of the remaining head words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing section 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# load sample article\r\n",
    "with open('database_dump_drugs/0.json') as file:\r\n",
    "    article = json.load(file)[0]['article_content'][:423]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# 1.1\r\n",
    "sentences = [sent for sent in sent_tokenize(article)]\r\n",
    "get_head_words(sentences=sentences)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['An Ethiopian foreign national', 0, 29],\n",
       " ['eMzinoni', 46, 54],\n",
       " ['drugs', 70, 75],\n",
       " ['his tuck shop', 79, 92],\n",
       " ['Friday, 6', 96, 105]]"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}