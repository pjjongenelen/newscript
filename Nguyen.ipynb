{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "\n",
    "from json import load\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from stanza import Pipeline\n",
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Head word extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_words(text, endpoint='8000'):\n",
    "    \"\"\"\n",
    "    Extracts single-noun head words from an article.\n",
    "\n",
    "    1) Extracts all noun phrases, tagged with 'NP' by the CoreNLPClient,\n",
    "    2) Removes multiples (NPs that contain other NPS),\n",
    "    3) Transforms multi-word NPs into single-word head nouns based on a simple Stanza Pipeline.\n",
    "    4) Converts the list to a general sentence ID + word ID representation\n",
    "\n",
    "    Parameters:\n",
    "    text (string): Article\n",
    "\n",
    "    Returns:\n",
    "    list: Head word sentence IDs & word IDs\n",
    "    \"\"\"\n",
    "\n",
    "    # extract all noun phrases from the article\n",
    "    with CoreNLPClient(properties='corenlp_server-2e15724b8064491b.props', endpoint=f'http://localhost:{endpoint}', memory='8G', be_quiet=True) as client:\n",
    "        matches = client.tregex(text=text, pattern='NP')\n",
    "    # reformat the data structure into a list of lists\n",
    "    noun_phrases = [[text, sent, begin, end] for text, sent, begin, end in\n",
    "                    zip([sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence],\n",
    "                        [sentence[match_id]['sentIndex'] for sentence in matches['sentences'] for match_id in sentence],\n",
    "                        [sentence[match_id]['characterOffsetBegin'] for sentence in matches['sentences'] for match_id in sentence],\n",
    "                        [sentence[match_id]['characterOffsetEnd'] for sentence in matches['sentences'] for match_id in sentence])]\n",
    "\n",
    "    # remove 'multiples'\n",
    "    _ = [np1 for np1 in noun_phrases for np2 in noun_phrases if (np1 != np2) and (np1[1] == np2[1]) and ((np1[2] <= np2[2]) and (np1[3] >= np2[3]))]\n",
    "    noun_phrases = [np for np in noun_phrases if np not in _]\n",
    "\n",
    "    # convert multi-word noun phrases into single-word head nouns\n",
    "    doc = nlp(text)\n",
    "    head_words = [[np[2] + np[0].find(word.text), np[2] + np[0].find(word.text) + len(word.text)] for np in noun_phrases for sent in nlp(np[0]).sentences for word in sent.words if word.deprel == \"root\"]\n",
    "\n",
    "    # convert the word indices into sentence ID and word ID pairs\n",
    "    head_words = [[sent.id, word.id] for sent in doc.sentences for word in sent.words for [i, j] in head_words if word.start_char == i and word.end_char == j]\n",
    "    \n",
    "    return head_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Trigger extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triggers(text, hw):\n",
    "    \"\"\"\n",
    "    Extracts head words that have either a verb or an eventive noun as its subject/object/preposition.\n",
    "\n",
    "    1) Extract all verbs\n",
    "    2) Get eventive nouns based on the Wordnet Synsets indicated by the authors\n",
    "    3) Combine 1 and 2 into a list of trigger candidates\n",
    "    4) Finds all head words that have a trigger as its subject/object/preposition.\n",
    "    5) Looks for transitive triggers, and extracts the correct subject from the root verb.\n",
    "\n",
    "    Parameters:\n",
    "    text (string): article\n",
    "    head_words (list): list of head word IDs\n",
    "\n",
    "    Returns:\n",
    "    list: Triplet lists of head word, trigger, and relation - e.g., [[4, 6, 'nsubj']]\n",
    "    \"\"\"\n",
    "\n",
    "    # parse the text and extract all verbs and lemmas\n",
    "    doc = nlp(text)\n",
    "    verbs = [[sent.id, word.id] for sent in doc.sentences for word in sent.words if word.upos == 'VERB']\n",
    "    lemmas = [[word.lemma, sent.id, word.id] for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "    # generate a list of eventive nouns\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        wn_evnouns = list(set([w for s in wn.synset('event.n.01').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "        wn_evnouns += list(set([w for s in wn.synset('act.n.02').closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "\n",
    "    # generates a list of trigger candidates based on the verbs and eventive nouns in the text\n",
    "    candidates = verbs + [[s,w] for [lemma, s, w] in lemmas if lemma in wn_evnouns and [s, w] not in verbs]\n",
    "\n",
    "    # finds all head word - trigger dyads and their syntactic relation\n",
    "    triggers = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if ([sent.id, word.head] in candidates) and ([sent.id, word.id] in hw):\n",
    "                if (\"IN\" in word.xpos):\n",
    "                    triggers.append([sent.id, word.id, word.head, word.xpos])\n",
    "                elif any(_ in word.deprel for _ in [\"subj\", \"obj\"]):\n",
    "                    triggers.append([sent.id, word.id, word.head, word.deprel])\n",
    "\n",
    "    for [sent, word] in verbs:\n",
    "        head = doc.sentences[sent].words[word-1].head\n",
    "        if head != 0:\n",
    "            for [_, noun, verb, rel] in triggers:\n",
    "                if verb == head and ('subj' in rel):\n",
    "                    triggers.append([sent, noun, word, rel])\n",
    "\n",
    "\n",
    "    return triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Attribute extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Add coreference information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general purpose testing variables\n",
    "nlp = Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse', verbose=False)  # stanza pipeline\n",
    "with open('database_dump_drugs/0.json') as file:\n",
    "    articles = list(set([a['article_content'] for a in load(file)]))\n",
    "article = articles[0]  # sample article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2],\n",
       " [0, 11],\n",
       " [0, 15],\n",
       " [0, 17],\n",
       " [0, 26],\n",
       " [1, 3],\n",
       " [1, 8],\n",
       " [1, 11],\n",
       " [1, 12],\n",
       " [1, 15],\n",
       " [1, 22],\n",
       " [1, 24],\n",
       " [1, 27],\n",
       " [1, 29],\n",
       " [1, 32],\n",
       " [1, 36],\n",
       " [1, 41],\n",
       " [2, 2],\n",
       " [2, 5],\n",
       " [2, 8],\n",
       " [2, 10],\n",
       " [3, 2],\n",
       " [3, 7],\n",
       " [3, 11],\n",
       " [3, 13],\n",
       " [4, 1],\n",
       " [4, 4],\n",
       " [4, 7],\n",
       " [5, 1],\n",
       " [5, 5],\n",
       " [5, 8],\n",
       " [5, 12],\n",
       " [5, 16],\n",
       " [5, 18],\n",
       " [6, 2],\n",
       " [6, 8],\n",
       " [6, 10],\n",
       " [7, 2],\n",
       " [7, 10],\n",
       " [7, 15],\n",
       " [7, 19],\n",
       " [8, 1],\n",
       " [8, 5],\n",
       " [8, 9],\n",
       " [8, 11],\n",
       " [8, 14],\n",
       " [9, 1],\n",
       " [9, 5],\n",
       " [9, 7],\n",
       " [9, 11],\n",
       " [9, 13],\n",
       " [10, 3],\n",
       " [10, 6],\n",
       " [10, 8],\n",
       " [10, 11],\n",
       " [10, 13],\n",
       " [10, 18],\n",
       " [10, 23],\n",
       " [11, 2],\n",
       " [11, 6],\n",
       " [11, 9],\n",
       " [11, 14],\n",
       " [11, 19],\n",
       " [11, 21],\n",
       " [12, 2],\n",
       " [12, 7],\n",
       " [12, 11],\n",
       " [12, 17],\n",
       " [12, 20],\n",
       " [12, 22],\n",
       " [13, 3],\n",
       " [13, 7],\n",
       " [13, 11],\n",
       " [13, 17],\n",
       " [14, 3],\n",
       " [14, 6],\n",
       " [14, 10],\n",
       " [14, 14],\n",
       " [14, 16],\n",
       " [14, 19],\n",
       " [14, 23],\n",
       " [14, 29],\n",
       " [15, 3],\n",
       " [15, 8],\n",
       " [15, 10],\n",
       " [15, 13],\n",
       " [15, 18],\n",
       " [15, 22]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing 1.1\n",
    "head_words = get_head_words(text=article)\n",
    "head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 3, 'nsubj'],\n",
       " [3, 2, 5, 'nsubj'],\n",
       " [3, 7, 9, 'nsubj'],\n",
       " [4, 1, 2, 'nsubj'],\n",
       " [5, 1, 3, 'nsubj'],\n",
       " [5, 5, 3, 'obj'],\n",
       " [5, 8, 11, 'nsubj'],\n",
       " [5, 12, 11, 'obj'],\n",
       " [5, 18, 14, 'obj'],\n",
       " [6, 2, 4, 'nsubj'],\n",
       " [6, 8, 6, 'obj'],\n",
       " [7, 2, 3, 'nsubj'],\n",
       " [7, 15, 16, 'nsubj'],\n",
       " [7, 19, 21, 'nsubj'],\n",
       " [8, 1, 3, 'nsubj'],\n",
       " [8, 5, 3, 'obj'],\n",
       " [9, 1, 2, 'nsubj'],\n",
       " [10, 3, 5, 'nsubj'],\n",
       " [10, 6, 5, 'obj'],\n",
       " [10, 13, 16, 'nsubj'],\n",
       " [10, 18, 16, 'obj'],\n",
       " [10, 23, 25, 'nsubj'],\n",
       " [11, 2, 3, 'nsubj'],\n",
       " [11, 6, 7, 'nsubj'],\n",
       " [11, 9, 7, 'obj'],\n",
       " [11, 21, 23, 'nsubj'],\n",
       " [12, 20, 18, 'nsubj'],\n",
       " [13, 3, 4, 'nsubj'],\n",
       " [13, 7, 8, 'nsubj'],\n",
       " [13, 11, 8, 'obj'],\n",
       " [13, 17, 15, 'obj'],\n",
       " [14, 3, 1, 'obj'],\n",
       " [14, 16, 18, 'nsubj'],\n",
       " [14, 19, 21, 'nsubj'],\n",
       " [14, 23, 21, 'obj'],\n",
       " [14, 29, 27, 'obj'],\n",
       " [15, 3, 4, 'nsubj'],\n",
       " [15, 13, 15, 'nsubj'],\n",
       " [15, 22, 24, 'nsubj'],\n",
       " [0, 6, 27, 'nsubj'],\n",
       " [3, 2, 9, 'nsubj'],\n",
       " [3, 3, 9, 'nsubj'],\n",
       " [5, 2, 11, 'nsubj'],\n",
       " [5, 1, 11, 'nsubj'],\n",
       " [5, 2, 11, 'nsubj'],\n",
       " [5, 1, 11, 'nsubj'],\n",
       " [5, 2, 11, 'nsubj'],\n",
       " [5, 8, 14, 'nsubj'],\n",
       " [5, 2, 14, 'nsubj'],\n",
       " [5, 1, 14, 'nsubj'],\n",
       " [5, 2, 14, 'nsubj'],\n",
       " [5, 1, 14, 'nsubj'],\n",
       " [5, 2, 14, 'nsubj'],\n",
       " [5, 20, 16, 'nsubj'],\n",
       " [5, 16, 16, 'nsubj'],\n",
       " [5, 20, 17, 'nsubj'],\n",
       " [5, 16, 17, 'nsubj'],\n",
       " [6, 2, 2, 'nsubj'],\n",
       " [6, 3, 2, 'nsubj'],\n",
       " [6, 3, 2, 'nsubj'],\n",
       " [6, 2, 6, 'nsubj'],\n",
       " [6, 3, 6, 'nsubj'],\n",
       " [6, 3, 6, 'nsubj'],\n",
       " [7, 2, 4, 'nsubj'],\n",
       " [7, 1, 4, 'nsubj'],\n",
       " [7, 2, 4, 'nsubj'],\n",
       " [7, 1, 4, 'nsubj'],\n",
       " [7, 2, 4, 'nsubj'],\n",
       " [7, 8, 16, 'nsubj'],\n",
       " [7, 2, 16, 'nsubj'],\n",
       " [7, 1, 16, 'nsubj'],\n",
       " [7, 2, 16, 'nsubj'],\n",
       " [7, 1, 16, 'nsubj'],\n",
       " [7, 2, 16, 'nsubj'],\n",
       " [7, 2, 21, 'nsubj'],\n",
       " [7, 1, 21, 'nsubj'],\n",
       " [7, 2, 21, 'nsubj'],\n",
       " [7, 1, 21, 'nsubj'],\n",
       " [7, 2, 21, 'nsubj'],\n",
       " [10, 23, 5, 'nsubj'],\n",
       " [10, 23, 16, 'nsubj'],\n",
       " [11, 2, 7, 'nsubj'],\n",
       " [11, 1, 7, 'nsubj'],\n",
       " [11, 2, 7, 'nsubj'],\n",
       " [11, 1, 7, 'nsubj'],\n",
       " [11, 2, 7, 'nsubj'],\n",
       " [11, 6, 17, 'nsubj'],\n",
       " [11, 2, 17, 'nsubj'],\n",
       " [11, 1, 17, 'nsubj'],\n",
       " [11, 2, 17, 'nsubj'],\n",
       " [11, 1, 17, 'nsubj'],\n",
       " [11, 2, 17, 'nsubj'],\n",
       " [11, 2, 23, 'nsubj'],\n",
       " [11, 1, 23, 'nsubj'],\n",
       " [11, 2, 23, 'nsubj'],\n",
       " [11, 1, 23, 'nsubj'],\n",
       " [11, 2, 23, 'nsubj'],\n",
       " [13, 2, 8, 'nsubj'],\n",
       " [13, 3, 8, 'nsubj'],\n",
       " [13, 3, 8, 'nsubj'],\n",
       " [13, 2, 8, 'nsubj'],\n",
       " [13, 1, 8, 'nsubj'],\n",
       " [13, 2, 8, 'nsubj'],\n",
       " [13, 1, 8, 'nsubj'],\n",
       " [13, 2, 8, 'nsubj'],\n",
       " [13, 8, 15, 'nsubj'],\n",
       " [13, 2, 15, 'nsubj'],\n",
       " [13, 1, 15, 'nsubj'],\n",
       " [13, 2, 15, 'nsubj'],\n",
       " [13, 1, 15, 'nsubj'],\n",
       " [13, 2, 15, 'nsubj'],\n",
       " [14, 20, 1, 'nsubj'],\n",
       " [14, 16, 1, 'nsubj'],\n",
       " [14, 20, 21, 'nsubj'],\n",
       " [14, 16, 21, 'nsubj'],\n",
       " [14, 21, 24, 'nsubj'],\n",
       " [14, 2, 24, 'nsubj'],\n",
       " [14, 1, 24, 'nsubj'],\n",
       " [14, 2, 24, 'nsubj'],\n",
       " [14, 1, 24, 'nsubj'],\n",
       " [14, 2, 24, 'nsubj'],\n",
       " [14, 22, 27, 'nsubj'],\n",
       " [14, 21, 27, 'nsubj'],\n",
       " [14, 2, 27, 'nsubj'],\n",
       " [14, 1, 27, 'nsubj'],\n",
       " [14, 2, 27, 'nsubj'],\n",
       " [14, 1, 27, 'nsubj'],\n",
       " [14, 2, 27, 'nsubj'],\n",
       " [15, 22, 4, 'nsubj'],\n",
       " [15, 21, 4, 'nsubj'],\n",
       " [15, 2, 4, 'nsubj'],\n",
       " [15, 1, 4, 'nsubj'],\n",
       " [15, 2, 4, 'nsubj'],\n",
       " [15, 1, 4, 'nsubj'],\n",
       " [15, 2, 4, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 3, 8, 'nsubj'],\n",
       " [15, 3, 8, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 1, 8, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 1, 8, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 22, 8, 'nsubj'],\n",
       " [15, 21, 8, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 1, 8, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 1, 8, 'nsubj'],\n",
       " [15, 2, 8, 'nsubj'],\n",
       " [15, 22, 15, 'nsubj'],\n",
       " [15, 21, 15, 'nsubj'],\n",
       " [15, 2, 15, 'nsubj'],\n",
       " [15, 1, 15, 'nsubj'],\n",
       " [15, 2, 15, 'nsubj'],\n",
       " [15, 1, 15, 'nsubj'],\n",
       " [15, 2, 15, 'nsubj']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing 1.2\n",
    "tr = get_triggers(text=article, hw=head_words)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: An 18-year-old man was found to be in possession of 16 cannabis deals by Garda, last week’s sitting of Portlaoise District Court heard.\n",
      "Before the court was Juraj Haenkovic (18) of 5 Carmody Way, Fairgreen, Portlaoise who was charged with possession of drugs, and possession of drugs for unlawful sale or supply on June 24 at 5 Carmody Way.\n",
      "The court heard the value of the drugs was €352.\n",
      "Defence solicitor Barry Fitzgerald said the defendant had moved to Ireland 14 years ago. He lives at home with his mother.\n",
      "Mr Fitzgerald told the court that his client had recently finished school, getting a graded Leaving Certificate.\n",
      "The accused now hopes to get an apprenticeship in carpentry.\n",
      "The defendant started using “weed” about a year ago and his “habit escalated”, Mr Fitzgerald said.\n",
      "Mr Fitzgerald told the court that the peer group the defendant is in uses drugs. He added that the amount of drugs was for the group of friends.\n",
      "While the defendant still has difficulties in relation to cannabis use, he has “reduced his habit fairly significantly”, Mr Fitzgerald said.\n",
      "The court herard that the defendant made full admissions whilst in Portlaoise Garda Station and has co-operated with gardai, Mr Fitzgerald said.\n",
      "“This is a very serious matter,” Judge Catherine Staines said, particularly as there were 16 deals for distribution.\n",
      "However, she noted that the defendant has no previous convictions and hopes to serve an apprenticeship.\n",
      "Putting the case back to January 21 of next year for a probation report, Judge Staines said this may involve the defendant undertaking counselling and providing urine samples.\n",
      "“If he becomes drug-free and deals with things, then I will deal with the issue leniently,” Judge Staines said.\n",
      "Head word: court\t Trigger: heard\t Relation: nsubj\n",
      "Head word: solicitor\t Trigger: said\t Relation: nsubj\n",
      "Head word: defendant\t Trigger: moved\t Relation: nsubj\n",
      "Head word: He\t Trigger: lives\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: told\t Relation: nsubj\n",
      "Head word: court\t Trigger: told\t Relation: obj\n",
      "Head word: client\t Trigger: finished\t Relation: nsubj\n",
      "Head word: school\t Trigger: finished\t Relation: obj\n",
      "Head word: Certificate\t Trigger: getting\t Relation: obj\n",
      "Head word: accused\t Trigger: hopes\t Relation: nsubj\n",
      "Head word: apprenticeship\t Trigger: get\t Relation: obj\n",
      "Head word: defendant\t Trigger: started\t Relation: nsubj\n",
      "Head word: habit\t Trigger: escalated\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: said\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: told\t Relation: nsubj\n",
      "Head word: court\t Trigger: told\t Relation: obj\n",
      "Head word: He\t Trigger: added\t Relation: nsubj\n",
      "Head word: defendant\t Trigger: has\t Relation: nsubj\n",
      "Head word: difficulties\t Trigger: has\t Relation: obj\n",
      "Head word: he\t Trigger: reduced\t Relation: nsubj\n",
      "Head word: habit\t Trigger: reduced\t Relation: obj\n",
      "Head word: Mr\t Trigger: said\t Relation: nsubj\n",
      "Head word: court\t Trigger: herard\t Relation: nsubj\n",
      "Head word: defendant\t Trigger: made\t Relation: nsubj\n",
      "Head word: admissions\t Trigger: made\t Relation: obj\n",
      "Head word: Mr\t Trigger: said\t Relation: nsubj\n",
      "Head word: deals\t Trigger: were\t Relation: nsubj\n",
      "Head word: she\t Trigger: noted\t Relation: nsubj\n",
      "Head word: defendant\t Trigger: has\t Relation: nsubj\n",
      "Head word: convictions\t Trigger: has\t Relation: obj\n",
      "Head word: apprenticeship\t Trigger: serve\t Relation: obj\n",
      "Head word: case\t Trigger: Putting\t Relation: obj\n",
      "Head word: Judge\t Trigger: said\t Relation: nsubj\n",
      "Head word: this\t Trigger: involve\t Relation: nsubj\n",
      "Head word: defendant\t Trigger: involve\t Relation: obj\n",
      "Head word: samples\t Trigger: providing\t Relation: obj\n",
      "Head word: he\t Trigger: becomes\t Relation: nsubj\n",
      "Head word: I\t Trigger: deal\t Relation: nsubj\n",
      "Head word: Judge\t Trigger: said\t Relation: nsubj\n",
      "Head word: was\t Trigger: heard\t Relation: nsubj\n",
      "Head word: solicitor\t Trigger: moved\t Relation: nsubj\n",
      "Head word: Barry\t Trigger: moved\t Relation: nsubj\n",
      "Head word: Fitzgerald\t Trigger: finished\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: finished\t Relation: nsubj\n",
      "Head word: Fitzgerald\t Trigger: finished\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: finished\t Relation: nsubj\n",
      "Head word: Fitzgerald\t Trigger: finished\t Relation: nsubj\n",
      "Head word: client\t Trigger: getting\t Relation: nsubj\n",
      "Head word: Fitzgerald\t Trigger: getting\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: getting\t Relation: nsubj\n",
      "Head word: Fitzgerald\t Trigger: getting\t Relation: nsubj\n",
      "Head word: Mr\t Trigger: getting\t Relation: nsubj\n",
      "Head word: Fitzgerald\t Trigger: getting\t Relation: nsubj\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17284/341332608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Input text: {article}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Head word: {doc.sentences[tr[x][0]].words[tr[x][1]-1].text}\\t Trigger: {doc.sentences[tr[x][0]].words[tr[x][2]-1].text}\\t Relation: {tr[x][3]}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# for demonstration purposes:\n",
    "doc = nlp(article)\n",
    "print(f'Input text: {article}')\n",
    "for x in range(len(tr)):\n",
    "    print(f'Head word: {doc.sentences[tr[x][0]].words[tr[x][1]-1].text}\\t Trigger: {doc.sentences[tr[x][0]].words[tr[x][2]-1].text}\\t Relation: {tr[x][3]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "529c65a099ec80885ec2eff43b42ddd85d3b5611c57bcaf738fb7ba7dbdb85b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
