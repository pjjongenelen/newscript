{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# imports\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import signal\r\n",
    "import subprocess\r\n",
    "import time\r\n",
    "\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "from stanza.server import CoreNLPClient"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def clear_port_9000():\r\n",
    "    # clear the 9000 port to be used by the CoreNLPClient\r\n",
    "    command = \"netstat -ano | findstr 9000\"\r\n",
    "    c = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr = subprocess.PIPE)\r\n",
    "    stdout, stderr = c.communicate()\r\n",
    "    if len(stdout) > 0:    \r\n",
    "        # process is running on port 9000, terminate it\r\n",
    "        print(\"Netstat response:\")\r\n",
    "        print(stdout)\r\n",
    "        print(\"-----------------------------------------------------\")\r\n",
    "        pid = input(\"Enter the process ID of the process that should be terminated, or an alphanumeric character in case it is unnecessary.\")\r\n",
    "        if not pid.isalpha():\r\n",
    "            os.kill(int(pid), signal.SIGTERM)\r\n",
    "            time.sleep(5)  # wait for the process to close"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open('database_dump_drugs/0.json') as file:\r\n",
    "    article = json.load(file)[0]['article_content'][:423]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sentences = [sent for sent in sent_tokenize(article)]\r\n",
    "\r\n",
    "clear_port_9000()\r\n",
    "with CoreNLPClient(properties='corenlp_server-2e15724b8064491b.props', memory='8G', be_quiet=True) as client:\r\n",
    "    matches = client.tregex(text=sentences[0], pattern='NP')  # finds the noun phrases in the given text\r\n",
    "\r\n",
    "# extract the noun phrases and their indices\r\n",
    "noun_phrases = [(text, begin, end) for text, begin, end in\r\n",
    "            zip([sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence],\r\n",
    "                [sentence[match_id]['characterOffsetBegin'] for sentence in matches['sentences'] for match_id in sentence],\r\n",
    "                [sentence[match_id]['characterOffsetEnd'] for sentence in matches['sentences'] for match_id in sentence])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Netstat response:\n",
      "b'  TCP    127.0.0.1:62869        127.0.0.1:9000         TIME_WAIT       0\\r\\n'\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 09:43:17 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\timjo\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-2e15724b8064491b.props -preload -outputFormat serialized\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "noun_phrases"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('An Ethiopian foreign national', 0, 29),\n",
       " ('eMzinoni', 46, 54),\n",
       " ('drugs at his tuck shop', 70, 92),\n",
       " ('drugs', 70, 75),\n",
       " ('his tuck shop', 79, 92),\n",
       " ('Friday, 6', 96, 105)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# find head words that contain other head words\r\n",
    "multiples = []\r\n",
    "for text1, i1, i2 in noun_phrases:\r\n",
    "    for text2, j1, j2 in noun_phrases:\r\n",
    "        if (text1 != text2) and (i1 != j1) and (i2 != j2):\r\n",
    "            if ((i1 >= j1) & (i2 < j2)) or ((i1 > j1) & (i2 <= j2)):\r\n",
    "                multiples.append(i2)\r\n",
    "set(multiples)\r\n",
    "\r\n",
    "# remove these 'multiples'\r\n",
    "# head_words = {key: val for key, val in noun_phrases.items() if val not in multiples}"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}